# ğŸš€ Sistema de Entrenamiento V2 - Rompiendo el Plateau de 84 Puntos

## ğŸ“Š DiagnÃ³stico del Problema Original

Tu sistema estaba estancado en 84 puntos debido a:

1. **Estado simplificado**: Solo 8 dimensiones â†’ poca informaciÃ³n
2. **Convergencia prematura**: Todos con LR=0.010, Î³=0.990
3. **MutaciÃ³n conservadora**: tasa=0.15 demasiado pequeÃ±a
4. **Pocas evaluaciones**: 50 episodios â†’ alta varianza
5. **Recompensas bÃ¡sicas**: No incentiva comportamientos intermedios

## ğŸ¯ Mejoras Implementadas en V2

### 1. Estado Mejorado (8 â†’ 12 dimensiones)

**Original:**
- Cuadrante cabeza (2 dims)
- RatÃ³n cercano direcciÃ³n/distancia (2 dims)
- RatÃ³n urgente direcciÃ³n/distancia (2 dims)
- Puntos + num ratones (2 dims)
**Total: 8 dimensiones**

**V2:**
- âœ… Todo lo anterior +
- âœ… Zona de pantalla (9 zonas en grid 3x3)
- âœ… Tendencia de puntos (subiendo/bajando/estable)
- âœ… Densidad de ratones (baja/media/alta)
- âœ… Amenaza inmediata (ratÃ³n a punto de escapar)
**Total: 12 dimensiones**

â†’ **33% mÃ¡s informaciÃ³n** para tomar decisiones

### 2. Reward Shaping (Recompensas Moldeadas)

**Original:**
```python
+10  captura ratÃ³n
-5   pierde ratÃ³n
-100 game over
```

**V2:**
```python
+20     captura ratÃ³n (aumentado!)
-10     pierde ratÃ³n (aumentado!)
+0.5    si ratÃ³n muy cerca (< 50 px)
+0.2    si ratÃ³n cerca (< 100 px)
-0.3    si ratÃ³n urgente (cerca del borde)
+1.0    bonificaciÃ³n por racha de capturas
+0.02   pequeÃ±a recompensa por sobrevivir
-200    game over (aumentado!)
```

â†’ **Incentiva comportamientos intermedios**, no solo el resultado final

### 3. MutaciÃ³n Adaptativa

**Original:**
```python
tasa_mutacion = 0.15  # Siempre fija
```

**V2:**
```python
tasa_base = 0.2

# Si estancado 10 generaciones
tasa = tasa_base * 2.0

# Si estancado 20 generaciones
tasa = tasa_base * 3.0

# Si detecta plateau
tasa = tasa_base * 1.5
```

â†’ **Aumenta exploraciÃ³n automÃ¡ticamente** cuando se detecta estancamiento

### 4. Diversidad Forzada

**Original:**
```python
# 1 elite + 5 mutaciones del mejor
nueva_poblacion = [mejor]
for i in range(5):
    mutante = mejor.mutar(0.15)
```

**V2 (cuando hay estancamiento):**
```python
# 1 elite
# 2 mutantes moderados
# 3 mutantes agresivos
# 2 COMPLETAMENTE ALEATORIOS
nueva_poblacion = [elite] + moderados + agresivos + aleatorios
```

â†’ **Evita convergencia prematura** con individuos completamente nuevos

### 5. MÃ¡s Evaluaciones

**Original:** 50 episodios
**V2:** 100 episodios

â†’ **Reduce varianza** en la evaluaciÃ³n, scores mÃ¡s confiables

### 6. PoblaciÃ³n MÃ¡s Grande

**Original:** 6 individuos
**V2:** 8 individuos

â†’ **Mayor diversidad** genÃ©tica

## ğŸ® CÃ³mo Usar

### OpciÃ³n 1: Empezar desde cero (recomendado)

```bash
python evolutionary_training_v2.py
```

Esto crearÃ¡ una carpeta `poblacion_v2/` con los modelos mejorados.

### OpciÃ³n 2: Migrar tu mejor modelo actual

Si quieres aprovechar lo que ya aprendiste:

```bash
# 1. Copiar tu mejor modelo a la nueva carpeta
mkdir -p poblacion_v2
cp snake_qlearning_best.pkl poblacion_v2/modelo_0.pkl

# 2. Ejecutar V2 (usarÃ¡ modelo_0 como base)
python evolutionary_training_v2.py
```

### Probar el modelo V2

```bash
# Modificar test_modelo.py para usar estado de 12 dims
python test_modelo.py snake_qlearning_best_v2.pkl
```

## ğŸ“ˆ QuÃ© Esperar

Con estas mejoras, deberÃ­as ver:

1. **Primeras 20 generaciones**: ExploraciÃ³n caÃ³tica, puede bajar
2. **Generaciones 20-50**: Comienza a superar 84 puntos
3. **Generaciones 50-100**: DeberÃ­a alcanzar 100-120 puntos
4. **Generaciones 100+**: Potencial de 150+ puntos

## ğŸ” Monitoreo

El sistema te avisarÃ¡:

```
âš ï¸  ESTANCAMIENTO: 15 generaciones sin mejora
   Aumentando mutaciÃ³n...
```

```
âš¡ MODO ANTI-ESTANCAMIENTO: MutaciÃ³n aumentada
```

Cuando veas estos mensajes, el sistema estÃ¡ **activamente** intentando romper el plateau.

## ğŸ›ï¸ Ajustes Opcionales

Si despuÃ©s de 100 generaciones todavÃ­a estÃ¡s estancado:

### Aumentar aÃºn mÃ¡s el estado

Editar `get_state()` para aÃ±adir:
- Velocidad de ratones
- Historia de Ãºltimas 3 acciones
- Distancia a los bordes de la pantalla

### Cambiar a Deep Q-Learning

Para estados muy complejos, considera usar una red neuronal en lugar de tabla Q:

```bash
# RequerirÃ¡ PyTorch o TensorFlow
pip install torch
```

(Puedo ayudarte a implementar esto si lo necesitas)

### Ajustar hiperparÃ¡metros

En el archivo `evolutionary_training_v2.py`:

```python
# LÃ­nea 20-23
POBLACION_SIZE = 12  # MÃ¡s diversidad (mÃ¡s lento)
EPISODIOS_ENTRENAMIENTO = 200  # MÃ¡s aprendizaje por generaciÃ³n
EPISODIOS_EVALUACION = 150  # EvaluaciÃ³n mÃ¡s precisa
```

## ğŸ†š ComparaciÃ³n de Rendimiento

| MÃ©trica | V1 Original | V2 Mejorado | Mejora |
|---------|-------------|-------------|---------|
| **Estado** | 8 dims | 12 dims | +50% |
| **PoblaciÃ³n** | 6 | 8 | +33% |
| **EvaluaciÃ³n** | 50 eps | 100 eps | +100% |
| **MutaciÃ³n** | Fija 0.15 | Adaptativa 0.2-0.6 | 2-4x |
| **Recompensas** | 3 tipos | 8 tipos | +167% |
| **Diversidad** | Baja | Forzada | âˆ |

## ğŸ’¡ PrÃ³ximos Pasos si Sigue Estancado

1. **Curriculum Learning**: Entrenar primero en versiÃ³n fÃ¡cil del juego
2. **Deep Q-Network**: Usar red neuronal en vez de tabla
3. **PPO/A3C**: Algoritmos mÃ¡s modernos que Q-Learning
4. **Imitation Learning**: Jugar tÃº mismo y que aprenda de ti

## ğŸ“ Necesitas Ayuda?

Si despuÃ©s de ejecutar V2 necesitas mÃ¡s optimizaciones, puedo:
- Implementar Deep Q-Learning
- Ajustar las recompensas especÃ­ficamente
- AÃ±adir mÃ¡s dimensiones al estado
- Visualizar la tabla Q para debugging
